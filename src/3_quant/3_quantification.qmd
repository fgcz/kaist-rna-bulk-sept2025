---
title: "Exercise #3: Quantification"
format:
  html:
    embed-resources: true
    toc: true
    toc-location: left
    toc-depth: 3
    title-block-banner: "#00A7FF"
filters:
  - flourish
css: report_assets/style.css
editor: visual
bibliography: report_assets/references.bib
---

# Bulk RNA-seq Quantification

In this tutorial, we look at the results of bulk RNA-seq quantification. In the first part, we look at how to generate counts from both STAR alignments using featureCounts and how to generate abundance estimates using Kallisto.

## Tools for Quantification

### Load required packages

```{r load packages}
suppressPackageStartupMessages({
  library(SummarizedExperiment)
  library(pheatmap)
  library(RColorBrewer)
  library(stringr)
  library(plotly)
  library(tidyverse)
})
```

### Feature Counts

First, let's use the input bam files from our subsampled dataset. These are available in the `~/work/gse154927-subsampled/bams` directory.

```{r}
# Check available bam files
bamFiles <- Sys.glob(file.path("~/work/gse154927-subsampled/bams", "*bam"))
print(bamFiles)
```

Second, we need to subset the annotation (gtf) file to include only protein-coding genes.

Do not execute the chunk below. It shows how you would filter the .gtf down to include the protein-coding genes only, but this takes some time for a human reference (5 minutes), so we skip it for now. There is a pre-generated gtf file with only the protein-coding transcripts in `~/work/grch38-p13-gencode-release-42/annotation/` which we will use instead.

```{r, eval=FALSE}
# Read the full GTF file
gtfFile <- "~/work/grch38-p13-gencode-release-42/annotation/genes.gtf"
gtf <- readGff(gtfFile)

# For human data, we'll filter for protein_coding genes
# Extract gene_biotype from attributes
gene_biotypes <- getGffAttributeField(gtf$attributes,
                                     field="gene_biotype", 
                                     attrsep="; *", valuesep=" ")

# Keep only protein_coding genes and exon features
gtf_filtered = gtf[gtf$type == "exon" & gene_biotypes %in% "protein_coding", ]

tmpOutDir <- "~/work/results/tmp"
# We create the tmp directory if it does not exist
dir.create(tmpOutDir, showWarnings=FALSE, recursive=TRUE)

# Save the subset GTF
gtfFileFiltered <- file.path(tmpOutDir, "genes_protein_coding.gtf")
write.table(gtf_filtered, gtfFileFiltered, quote=FALSE, sep="\t", row.names=FALSE, col.names=FALSE)

rm(list=c("gtf", "gtf_filtered", "gene_biotypes"))
gc()
```

#### Running `featureCounts`

We are now ready to run `featureCounts` from the Rsubread package.

```{r}
#| class-output: scrolling
source("helpers.R")

bamFiles <- Sys.glob(file.path("~/work/gse154927-subsampled/bams", "*bam"))
outputDir <- "~/work/results/3_quant/featureCounts"
dir.create(outputDir, showWarnings=FALSE, recursive=TRUE)

for (bamFile in bamFiles[4]) {
  sampleName <- tools::file_path_sans_ext(basename(bamFile))
  outputPrefix <- file.path(outputDir, sampleName)
  
  countResult = Rsubread::featureCounts(
    bamFile, 
    annot.inbuilt=NULL,
    annot.ext="~/work/grch38-p13-gencode-release-42/annotation/genes_protein_coding.gtf",
    isGTFAnnotationFile=TRUE,
    GTF.featureType="exon",
    GTF.attrType="gene_id",
    useMetaFeatures=TRUE,
    allowMultiOverlap=TRUE,
    isPairedEnd=TRUE,
    nthreads=4, 
    countMultiMappingReads=TRUE,
    strandSpecific=0,
    minMQS=10,
    minOverlap=10,
    primaryOnly=TRUE
  )
  colnames(countResult$counts) = "matchCounts"
  cnts <- countResult$counts %>% as.data.frame() %>% rownames_to_column("Identifier")
  data.table::fwrite(cnts, file=paste0(outputPrefix, ".txt"), sep="\t")
  colnames(countResult$stat) = c("Status", "Count")
  sts <- countResult$stat %>% as.data.frame()
  data.table::fwrite(sts, file=paste0(outputPrefix, "-stats.txt"), sep="\t")
}
```

### Running Kallisto

Next, we will perform the quantification using Kallisto and see how its performance compares against STAR. As Kallisto is a command-line tool, all code in this section will be bash-based.

#### Use trimmed data

We will use the fastp-processed data that's available in our subsampled dataset directory.

```{bash}
#| class-output: scrolling

# Check available trimmed data
ls -la ~/work/gse154927-subsampled/fastp/
```

Now that we have the data, we are ready to perform the quantification with Kallisto. We'll need to build a Kallisto index first from our filtered GTF, then run quantification. Note that for this tutorial, we'll need to extract transcripts from our GTF file first.

```{bash}
#| class-output: scrolling

# We can create a Kallisto index using the following command
# kallisto index -i transcripts.idx ~/work/grch38-p13-gencode-release-42/annotation/transcripts_protein_coding.fa

echo "Note: For this tutorial, we'll use a pre-built transcriptome index"

KALLISTO_INDEX=~/work/grch38-p13-gencode-release-42/annotation/genes_protein_coding_kallistoIndex/transcripts.idx

FASTP_OUTDIR=~/work/gse154927-subsampled/fastp
KALLISTO_OUTDIR=~/work/results/3_quant/kallisto
mkdir -p $KALLISTO_OUTDIR

ALL_SAMPLES=$(cat ~/work/gse154927-subsampled/fastq/dataset.tsv | awk -F"\t" '{print $1}' | tail +2 | sort)

# We use the same 
for SAMPLE_NAME in HCT116_EpCAMhigh_1 HCT116_EpCAMhigh_2 HCT116_EpCAMhigh_3 HCT116_EpCAMhigh_4
  do FASTQ_R1="${FASTP_OUTDIR}/${SAMPLE_NAME}_R1.fastq.gz"
  FASTQ_R2="${FASTP_OUTDIR}/${SAMPLE_NAME}_R2.fastq.gz"
  echo $SAMPLE_NAME
  time kallisto quant \
    -i $KALLISTO_INDEX \
    -o $KALLISTO_OUTDIR/$SAMPLE_NAME -t 4 --bootstrap-samples 10 --seed 42 \
    $FASTQ_R1 $FASTQ_R2 \
    2> $KALLISTO_OUTDIR/$SAMPLE_NAME"_kallisto.stderr" > $KALLISTO_OUTDIR/$SAMPLE_NAME"_kallisto.stdout"
done
```

#### Exercise #1

1.  Did you notice a difference between the execution time of featureCounts versus Kallisto? Are the execution times comparable?

2.  Take a moment to look at the outputs of FeatureCounts. What information is provided in the summary statistics files?

3.  In what format are the counts outputted by featureCounts? What influence could this possibly have on downstream analysis?

```{r}
# Let's examine the featureCounts output
fcFiles <- Sys.glob("~/work/results/3_quant/featureCounts/*.txt")
if(length(fcFiles) > 0) {
  # Look at the first file
  firstSample <- data.table::fread(fcFiles[1])
  print("FeatureCounts output structure:")
  print(head(firstSample))
  print(paste("Number of genes:", nrow(firstSample)))
  print(paste("Data type of counts:", class(firstSample$matchCounts)))
  
  # Look at summary stats
  statsFiles <- Sys.glob("~/work/results/3_quant/featureCounts/*-stats.txt")
  if(length(statsFiles) > 0) {
    stats <- data.table::fread(statsFiles[1])
    print("Summary statistics:")
    print(stats)
  }
}
```

::: {.callout-tip appearance="simple" collapse="true"}
##### Solutions

1.  FeatureCounts typically runs quite quickly on subsampled data like this. Kallisto would be faster for transcript-level quantification, but both tools perform well on datasets of this size. Keep in mind that featureCounts requires .bam files which we generated in a previous step with STAR, while Kallisto works on the fastqs themselves. Alignment with STAR can take a long time, especially with large genomes.

2.  The summary statistics show important information including:

-   Total number of reads processed
-   Successfully assigned reads
-   Unassigned reads due to various reasons (multimapping, no features, etc.)
-   This information is crucial for assessing the quality of your quantification

3.  FeatureCounts outputs integer counts, which is appropriate for most differential expression analysis tools that expect discrete count data. This contrasts with some other quantification methods that might output continuous values that need to be rounded.
:::

## CountQC

Now that we know how to generate counts based on some downsampled data, let's move into how to QC counts. For this, we switch again to the **full data**, which has been processed for us.

### Loading counts

Let's fetch the input count files generated from featureCounts for each sample.

```{r setup}
countDirectoryToUse <- "~/work/gse154927-full/featureCounts"
countFilenames <- list.files(path="~/work/gse154927-full/featureCounts", pattern="*[0-9].txt")
```

You will notice a new directory called "feature_counts" in the same folder where you executed this code. Take a quick look inside.

How were these counts generated?

-   Trimming with `fastp`

-   Alignment with `STAR` to the GRCh38.p13 reference genome

-   Running featureCounts from the Rsubread package using the GENCODE Release 42 GTF file. The file used can be found here on our [public FTP](https://fgcz-gstore.uzh.ch/public/KAIST_SS25/GRCh38.p13_GENCODE_Release_42/annotation/)

The file is quite large and you need not be familiar with its format at this point or download it. What is important to know is that this file has been subset to only the protein_coding genes.

There are a few reasons why we might rather exclude transcript types such as lncRNA and snRNA.

-   Including these other transcript types increase the number of tests, meaning more false positives, and less power to detect protein-coding genes

-   non-coding RNAs do not have poly-A tails and can't be detected with mRNA sequencing, so their absence might be misinterpreted

-   They are sometimes not well annotated and this can adversely affect enrichment analysis

In general, a good strategy is to exclude them by default and include them by demand (i.e. if the research question explicitly demands it).

Now that the we have loaded the counts, we can start exploring the data. We could proceed with either the counts generated by Kallisto or featureCounts. Let's load the featureCounts for now.

```{r metadata}
# Define meta dataframe for later use
sampleNames <- tools::file_path_sans_ext(countFilenames)
meta <- data.frame(
  Condition=as.factor(c(rep("HCT116_EpCAMhigh", 4), 
                        rep("HCT116_EpCAMlow", 4),
                        rep("SW480_EpCAMhigh", 4),
                        rep("SW480_EpCAMlow", 4))),
  row.names=sampleNames
)

# Define some general-use parameters for use later
sigThresh <- 10  # significane threshold for the counts
conditionColours <- scales::hue_pal()(length(unique(meta$Condition)))  # choose pretty colours
# associate the conditions with the pretty colours
names(conditionColours) <- unique(meta$Condition)
sampleColours <- conditionColours[meta$Condition]
```

First, we load the counts and combine them into a single data-frame. Note we do this here manually, but there are methods like the `tximport` function which perform this operation in a convenient way, and beyond this, correct for biases in the counts. We will be using this method of import in the optional section on differential expression analysis.

```{r load counts, message=FALSE}
# Find the count files on the system
countPaths <- Sys.glob(file.path(countDirectoryToUse, sprintf("*%s.txt", rownames(meta))))
names(countPaths) <- rownames(meta)

# Load into memory and combine
countList <- lapply(rownames(meta), function(sn) {
  count <- vroom::vroom(countPaths[[sn]]) %>% 
    as.data.frame() %>%
    tibble::column_to_rownames(var="Identifier") %>%
    dplyr::rename_with(~sn)
  return(count)
})
rawCounts <- bind_cols(countList)

# Save merged data for later use
outputDirectory <- "~/results/3_quant"
dir.create(outputDirectory, showWarnings = FALSE, recursive = TRUE)
saveRDS(rawCounts, 
        file=file.path(outputDirectory, "mergedCounts.rds"))
```

### Count Statistics

Let's look at the count statistics first. In the first of two plots, we get the total counts for each sample. In the second, we see how many features, and what fraction of all features, exceed our signal threshold.

```{r barplots}
toPlot <- tibble(
  Sample=colnames(rawCounts),
  `Read Counts`=colSums(rawCounts),
  `Fraction of features above threshold`=colSums(rawCounts > sigThresh),
  Percentages=paste(signif(100 * colSums(rawCounts > sigThresh) / nrow(rawCounts), digits=2), "%")
)
plot_ly(
  toPlot,
  x = ~Sample, 
  y = ~`Read Counts`, 
  type = "bar"
) %>%
  layout(title="Total reads", yaxis = list(title = "Counts [Mio]"))
plot_ly(
  toPlot, 
  x = ~Sample, 
  y = ~`Fraction of features above threshold`, 
  type="bar",
  text=~Percentages, textposition = 'auto'
) %>%
  layout(title="Genomic features with reads above threshold", yaxis = list(title = "Count"))
```

### Exercise #2

1.  How do the read counts compare to the initial FastQC report? See the plot below.

![Raw fastq read counts for each sample](report_assets/fastq_counts.png)

2.  How many features (in %) are above threshold? Do you think re-sequencing the samples with relatively low counts will be useful or provide a significant return on investment?

::: {.callout-note appearance="simple" collapse="true"}
#### Solutions

1.  The counts after mapping and quantification are lower.

2.  All around 50-60%. Re-sequencing is unlikely to be worth our time, unless we are interested in very lowly-expressed genes. Even the sample with the lowest counts has almost as many features expressed above threshold as the sample with the highest read counts, despite having only a fraction of the counts.
:::

### Filtering Counts

Before continuing, we should filter out genes which are lowly expressed or absent, which we do on a per-group basis (think about why this might be). Next, we generate normalised counts using "Variance Stabilisation" as provided in the DESeq2 package. As per the documentation, this function calculates a variance stabilizing transformation (VST) from the fitted dispersion-mean relation(s) and then transforms the count data (normalized by division by the size factors or normalization factors), yielding a matrix of values which now have constant variance along the range of mean values (homoskedastic)[@DESeq2]. There are other functions we could use, such as `rlog` which is less sensitive to size factors, but VST works well in the general case.

First though, we want to filter the count matrix for genes which are informative, i.e. which are sufficiently expressed in the dataset and which can stratify our data. Before, we defined a minimum threshold of `r sigThresh` at which we consider a gene expressed in a sample. Below, we do some calculations to determine if a gene is sufficiently expressed in our dataset at this treshold.

A simple approach would be to say that we want the gene expressed in at least, for example, 25% of the samples. However, this simple approach might let some genes through which are in fact just noise in our data. Would we for example consider a gene to be important if it is expressed in only a single replicate for every group? Probably not. So we can try something else.

Our strategy:

1.  Within a group, do at least half the samples express the gene? If yes, we consider the gene expressed in that group.

2.  Do at least half the groups express the gene? If yes, we keep it. If no, we toss it.

For our data, this means that we keep a gene if it is expressed in 2 or more samples within 2 or more groups.

```{r filtering}
# Which genes are above threshold?
isPresent <- rawCounts > sigThresh
# For every gene, how many samples within a group have it expressed?
isPresentCond <- rowsum(t(isPresent * 1), group=meta$Condition)
# For every gene for a particular group, is it expressed in at half the samples?
isPresentCond <- t(sweep(isPresentCond, 1,
                         table(meta$Condition)[rownames(isPresentCond)], FUN="/")) >= 0.5
# For every gene, is it expressed in at least 2 groups?
isValid <- rowMeans(isPresentCond) >= 0.5
# Subset the count matrix
rawCountsFilt <- rawCounts[isValid, ]
```

Now that we have filtered the genes, let's see how the distribution of counts looks before and after the filtration step.

```{r filtering before after}
# Let's compare raw counts and filtered raw counts:
density_data <- density(log1p(as.matrix(rawCounts)), na.rm = TRUE)  # log1p to handle zeros
plot_ly(
  x = density_data$x,
  y = density_data$y,
  type = "scatter",
  mode = "lines",
  name = "All Samples",
  line = list(color = "blue")
) %>%
  layout(
    title = "Density Plot of Log-Transformed Raw Gene Expression Counts",
    xaxis = list(title = "log1p(Counts)"),
    yaxis = list(title = "Density")
  )

density_data <- density(log1p(as.matrix(rawCountsFilt)), na.rm = TRUE)  # log1p to handle zeros
plot_ly(
  x = density_data$x,
  y = density_data$y,
  type = "scatter",
  mode = "lines",
  name = "All Samples",
  line = list(color = "blue")
) %>%
  layout(
    title = "Density Plot of Log-Transformed Filtered Raw Gene Expression Counts",
    xaxis = list(title = "log1p(Counts)"),
    yaxis = list(title = "Density")
  )
```

With the genes filtered, we will now perform normalisation. We have many options here. As we will be using DESeq2 to do the differential expression in the next chapter, let's also use it here to perform the normalisation.

```{r}
# Load data into a DESeq2 dataset so we can use the variance stabilizing function from DESeq2
dds <- DESeq2::DESeqDataSetFromMatrix(countData=rawCountsFilt,
                                      colData=meta,
                                      design=~Condition)
vsd <- DESeq2::vst(dds)

# Extract normalized counts
vsdSE <- SummarizedExperiment::assay(vsd)
```

Let's quickly compare the raw and normalised counts to see how they have changed.

```{r head counts}
head(rawCountsFilt)
normCounts <- as.data.frame(vsdSE)
head(normCounts)

# Compare boxplots of raw and normalized counts
data_long <-  rawCountsFilt %>%
  pivot_longer(cols = everything(),
               names_to = "Sample", 
               values_to = "Count")
data_long_log <- data_long %>%
  mutate(LogCount = log2(Count + 1))# Get long format and +1 to handle possible zeros in log scale

plot_ly(data_long_log, 
              x = ~Sample, 
              y = ~LogCount, 
              type = "box",
              boxpoints = "outliers") %>%
  layout(title = "Distribution of Log(Counts+1) Across Samples",
         xaxis = list(title = "Samples", 
                     tickangle = -45),
         yaxis = list(title = "Log(Counts + 1)"),
         margin = list(b = 100))


data_long <-  normCounts %>%
  pivot_longer(cols = everything(),
               names_to = "Sample", 
               values_to = "Count")
data_long_log <- data_long %>%
  mutate(LogCount = log2(Count + 1))  # Get long format and +1 to handle possible zeros in log scale

plot_ly(data_long_log, 
              x = ~Sample, 
              y = ~LogCount, 
              type = "box",
              boxpoints = "outliers") %>%
  layout(title = "Distribution of Log2(Normalized counts+1) Across Samples",
         xaxis = list(title = "Samples", 
                     tickangle = -45),
         yaxis = list(title = "Log2(Normalized Counts + 1)"),
         margin = list(b = 100))

```

#### Dimensionality Reduction

We now have a count matrix of with the following dimensions: 16 x 14'392. In other words, we have represented each sample by a vector of 14'392 features. In order to adequately QC our samples, one important aspect is determining how similar or dissimilar one vector of genes from a specific sample is to that of another. However, interpreting 14'392 dimensions directly is not something that humans are generally capable of doing. This is where dimensionality reduction comes in. We would ideally like to *reduce* the given *dimensions* (a 14'392-dimensional space), down to a 2- or 3-dimensional space, so we can visualise the result.

Two approaches, which are in fact related, are PCA (*principal component analysis*) and MDS (*multidimensional scaling*).

#### PCA

PCA is a linear dimensionality reduction technique which, in essence, tries to find a rotation of the data in order to maximise the variance. This is a common method for dimensionality reduction across many different disciplines within Computer Science and beyond. We will use the built-in R-method `prcomp` below to calculate the principal components and manually calculate the variance explained by each component.

Additionally, we will plot a 'scree' plot, which aims to visualise the variance explained by each principal component.

```{r PCA scree}
# Run PCA
pcDat  <- prcomp(t(vsdSE), scale. = FALSE)

# Calculate explained variance
varExp <- (100*pcDat$sdev^2)/sum(pcDat$sdev^2)

# Store the explained variance of top 8 PCs
varExp_df <- data.frame(PC= paste0("PC",1:8),
                          varExp=varExp[1:8])

# Scree plot
ggplot(varExp_df, aes(x=PC,y=varExp, group=1)) +
geom_point(colour="steelblue", size=4) +
geom_col(fill="steelblue") +
geom_line() + 
theme_bw() + ylim(c(0,100))
```

Let's now plot the first and second principal components in 2-dimensions.

```{r PCA plot}
plot_ly(as.data.frame(pcDat$x), 
        x=~PC1, y=~PC2, 
        color=meta$Condition, 
        colors="Set1",
        type="scatter", 
        mode="markers") %>%
  layout(title="PCA Plot")
```

#### Multi-dimensional scaling (MDS)

Multi-dimensional scaling is another dimensionality reduction which aims to best reconstruct pairwise distances between a set of points given a set of distances. Since in the case of RNA-seq we are given the data vectors directly rather than the distance matrices, methods must calculate the distance matrix first upon which the MDS algorithm is then performed. PCA is used in the process to produce a reduced dimensionality projection from the similarities.

Here, we use limma [@limma] to calculate the MDS and use plotly to visualise the pairwise distances in 3-dimensions.

```{r MDS plot}
mds <- limma::plotMDS(vsdSE, plot=FALSE)
mdsOut <- mds$eigen.vectors[,1:3]
colnames(mdsOut) <- c("Leading logFC dim1", 
                      "Leading logFC dim2", 
                      "Leading logFC dim3")
toPlot <- cbind(meta %>% rownames_to_column("Sample"), mdsOut)
plot_ly(toPlot, 
        x=~`Leading logFC dim1`, 
        y=~`Leading logFC dim2`, 
        z=~`Leading logFC dim3`, 
        color=~Condition, 
        colors="Set1", 
        type='scatter3d', 
        mode='markers+text', 
        text=~Sample, 
        textposition = "top right") %>%
  plotly::layout(title="Classical MDS", 
                 scene=list(xaxis=list(title = 'Leading logFC dim1'), 
                            yaxis = list(title = 'Leading logFC dim2'), 
                            zaxis = list(title = 'Leading logFC dim3')))
```

::: {.callout-note appearance="simple" collapse="true"}
#### PCA vs MDS

The following image and linked StackedOverflow post explains well the difference between the two methods.

[![](https://i.stack.imgur.com/WvnU7.png){fig-alt="Visual explanation of the  difference between PCA and MDS" fig-align="center"}](https://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-component-analysis-and-multidimensional)
:::

### Exercise #2

1.  How many genes are used by the function `plotMDS` as inputs. What kind of genes are these?
2.  Change the above function call so that all genes in the count matrix are used as input.

::: {.callout-tip collapse="true" appearance="simple"}
#### Solutions

1.  By default, the top 500 genes are used.

2.  We must change the `top` argument in order to include all necessary genes.

    ```{r, eval=FALSE}
    mds <- limma::plotMDS(vsdSE, top=nrow(vsdSE), plot=FALSE)
    ```
:::

### Correlation Plots

We can also quantify the similarity between samples using correlations. This involves simply computing pairwise correlations between samples using the gene count vectors of each samples as inputs. The following methods this method as a basis.

#### By-Sample Dendogram and Correlations

We can use hierarchical clustering using the pearson correlations as inputs. This will group samples together in a hierarchical fashion in a tree-like structure.

```{r dendogram}
d <- as.dist(1-cor(vsdSE, use="complete.obs"))   # calculate the pairwise correlations
hc <- hclust(d, method="ward.D2")  # run the clustering algorithm
WGCNA::plotDendroAndColors(hc, sampleColours, autoColorHeight=TRUE, hang = -0.1)
```

#### Correlation Heatmap

Similarly, we can call `pheatmap` on the correlations to easily visualize the similarity between samples within and across conditions.

```{r correlation heatmap, fig.width=12, fig.height=12}
# Pearson correlation plot 
pheatmap(
  mat               = cor(vsdSE, use="complete.obs"),
  treeheight_row    = 100,
  treeheight_col    = 100,
  cutree_rows       = 2, 
  cutree_cols       = 2,
  silent            = F,
  annotation_col    = meta,
  annotation_colors = list(Condition = conditionColours),
  color             = brewer.pal(n = 9, name = "Blues"),
  fontsize_row      = 12, 
  fontsize_col      = 12,
  display_numbers   = TRUE,
  fontsize_number   = 12)
```

#### Top 2'000 Variable Genes Heatmap

```{r pheatmap 2k variable}
# First, we center the matrix
vsdSECentered <- sweep(vsdSE, 1, rowMeans(vsdSE))
# Identify high variance features
topGenes <- rownames(vsdSE)[head(order(rowSds(vsdSE, na.rm=TRUE),
                                            decreasing = TRUE), 2000)]

countsToPlot <- vsdSECentered[topGenes,]

# Clustering of high variance features
hmObj <- pheatmap(countsToPlot, 
         clustering_method="ward.D2",
         scale = "row", cluster_rows = TRUE,
         cluster_cols = TRUE, show_rownames = FALSE,
         cutree_rows = 6, cutree_cols = 2,
         treeheight_row = 50, treeheight_col = 50,
         annotation_col = meta,
         fontsize_row = 8, fontsize_col = 9,
         annotation_legend = TRUE,
         fontsize=8)
hmObj
```
